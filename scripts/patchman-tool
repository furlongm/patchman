#!/usr/bin/env python

from django.core.management import setup_environ

import os
import sys
import string
import bz2
import gzip
import StringIO
import re
import hashlib
import datetime
import argparse
import math
from StringIO import StringIO
from lxml import etree
from debian.debian_support import Version
from debian.deb822 import Sources
from urllib2 import Request, urlopen

sys.path.append('/usr/local/src')

os.environ['DJANGO_SETTINGS_MODULE'] = 'patchman.settings'

from patchman import settings
setup_environ(settings)

from django.db.utils import IntegrityError
from django.db import connection
from django.db.models import Q, Count
from django.dispatch import Signal

from patchman.hosts.models import Host
from patchman.operatingsystems.models import OS, OSGroup
from patchman.domains.models import Domain
from patchman.packages.models import Package, PackageName, PackageString, PackageUpdate
from patchman.repos.models import Repository, RepoPackage
from patchman.arch.models import PackageArchitecture, MachineArchitecture
from patchman.reports.models import Report

from patchman.hosts.signals import host_update_found

from progressbar import Bar, ETA, Percentage, ProgressBar

force = False
verbose = False

def progress_bar(ptext, plength):

    jtext = string.ljust(ptext, 35)
    pbar = ProgressBar(widgets=[jtext, Percentage(), Bar(), ETA()], maxval=plength).start()
    return pbar

def create_pbar(ptext, plength):

    if verbose and plength > 0:
        return progress_bar(ptext, plength)
    else:
        return None

def update_pbar(pbar, i):

    if verbose and pbar:
        pbar.update(i)

def gunzip(contents):

    try:
        gzipdata = gzip.GzipFile(fileobj=contents)
        gzipdata = gzipdata.read()
        contents = StringIO(gzipdata)
    except IOError, e:
        import warnings
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        if e.message == 'Not a gzipped file':
            pass

    return contents.getvalue()

def bunzip2(contents):

    try:
        bzip2data = bz2.decompress(contents)
        return bzip2data
    except IOError, e:
        if e == 'invalid data stream':
            pass

def extract(data):

    extracted = bunzip2(data)
    if not extracted:
        extracted = gunzip(StringIO(data))
    return extracted

def unpack_debpackages(data, packages):

    extracted = extract(data)
    package_re = re.compile('^Package: ', re.M)
    numpackages = len(package_re.findall(extracted))

    if numpackages > 0:
        pbar = create_pbar('Extracting packages: ', numpackages)
        for i, stanza in enumerate(Sources.iter_paragraphs(StringIO(extracted))):
            fullversion = Version(stanza['version'])
            arch = stanza['architecture']
            name = stanza['package']
            epoch = fullversion._BaseVersion__epoch
            if epoch == None:
                epoch = ''
            version = fullversion._BaseVersion__upstream_version
            release = fullversion._BaseVersion__debian_revision
            if release == None:
                release = ''
            update_pbar(pbar, i+1)
            package = PackageString(name=name, epoch=epoch, version=version, release=release, arch=arch, packagetype='D')
            packages.add(package)
            del package
    else:
        if verbose:
            print 'No packages in found in repo.'

    del data, extracted

def get_primary_url(repo_url, data):

    extracted = extract(data)

    ns='http://linux.duke.edu/metadata/repo'
    context = etree.parse(StringIO(data), etree.XMLParser())
    location = context.xpath("//ns:data[@type='primary']/ns:location/@href",  namespaces = {'ns': ns})[0]
    checksum = context.xpath("//ns:data[@type='primary']/ns:checksum",  namespaces = {'ns': ns})[0].text
    primary_url = str(repo_url.rsplit('/', 2)[0]) + '/' + location
    return primary_url, checksum

def get_sha1(data):

    return hashlib.sha1(data).hexdigest()

def unpack_rpmpackages(data, packages):

    extracted = extract(data)
    
    ns='http://linux.duke.edu/metadata/common'
    context = etree.iterparse(StringIO(extracted), tag = '{%s}metadata' % ns)
    numpackages = int(context.next()[1].get('packages'))
    context = etree.iterparse(StringIO(extracted), tag = '{%s}package' % ns)

    if numpackages > 0:

        pbar = create_pbar('Extracting packages: ', numpackages)

        for i, data in enumerate(context):
            event = data[0]
            elem = data[1]
            update_pbar(pbar, i+1)
            name = elem.xpath('//ns:name', namespaces = {'ns': ns})[0].text.lower()
            arch = elem.xpath('//ns:arch', namespaces = {'ns': ns})[0].text
            fullversion = elem.xpath('//ns:version', namespaces = {'ns': ns})[0]
            epoch = fullversion.get('epoch')
            version = fullversion.get('ver')
            release = fullversion.get('rel')

            elem.clear()
            while elem.getprevious() is not None:
                del elem.getparent()[0]
    
            if name != '' and version != '' and arch !='':
                if epoch == '0':
                    epoch = ''
                package = PackageString(name=name, epoch=epoch, version=version, release=release, arch=arch, packagetype='R')
                packages.add(package)
        del context
        del data

    else:
        if verbose:
            print 'No packages in found in repo.'

def get_repo_url(url):

    try:
        req = Request(url)
        return urlopen(req)
    except IOError, e:
        if hasattr(e, 'reason'):
            print 'Failed to reach the server: %s' % e.reason
            return -1
        elif hasattr(e, 'code'):
            if verbose:
                print url
                print '%s - This may not be fatal, check below.' % e
            return e.code 
        else:
            print 'Error: %s' % e
            return -1

def update_packages(repo, packages):

    new = set()
    old = set()
    removed = set()

    repopackages = repo.packages.all()
    reposize = len(repopackages)

    pbar = create_pbar('Obtaining stored packages: ', reposize)
    for i, package in enumerate(repopackages):
        update_pbar(pbar, i+1)
        name=str(package.name)
        arch=str(package.arch)
        strpackage = PackageString(name=name, epoch=package.epoch, version=package.version, release=package.release, arch=arch, packagetype=package.packagetype)
        old.add(strpackage)

    new = packages.difference(old)
    removed = old.difference(packages)

    old.clear()
    del old

    newlen = len(new)
    removedlen = len(removed)

    pbar = create_pbar('Removing %s obsolete packages:' % removedlen, removedlen)
    for i, package in enumerate(removed):
        update_pbar(pbar, i+1)
        package_id = PackageName.objects.get(name=package.name)
        epoch = package.epoch
        version = package.version
        release = package.release
        arch = PackageArchitecture.objects.get(name=package.arch)
        packagetype = package.packagetype
        p = Package.objects.get(name=package_id, epoch=epoch, version=version, arch=arch, release=release, packagetype=packagetype)
        RepoPackage.objects.get(repo=repo, package=p).delete()
        del p
    repo.save()
    removed.clear()

    pbar = create_pbar('Adding %s new packages:' % newlen, newlen)
    for i, package in enumerate(new):
        update_pbar(pbar, i+1)
        package_id, c = PackageName.objects.get_or_create(name=package.name)
        epoch = package.epoch
        version = package.version
        release = package.release
        packagetype = package.packagetype
        arch, c = PackageArchitecture.objects.get_or_create(name=package.arch)
        p, c = Package.objects.get_or_create(name=package_id, epoch=epoch, version=version, arch=arch, release=release, packagetype=packagetype)
        RepoPackage.objects.create(repo=repo, package=p)
    repo.save()
    new.clear()

    del new
    del removed
    del repopackages

def find_repo_url(stored_repo_url, formats):

    for fmt in formats:
        repo_url = stored_repo_url
        for f in formats:
            repo_url = repo_url.rstrip(f)
        repo_url  = repo_url.rstrip('/') + '/' + fmt
        res = get_repo_url(repo_url)
        if type(res) != int:
            break
    return repo_url, res

def download_url(res):

    headers = dict(res.headers.items())
    if verbose and 'content-length' in headers:
        ptext = 'Downloading repository info:'
        contentlen = int(headers['content-length'])
        chunk_size = 16384.0
        pbar = progress_bar(ptext, contentlen)
        i = 0
        chunks = int(math.ceil(contentlen / chunk_size))
        data = ''
        chunk = ''
        for x in range(1, chunks + 1):
            chunk = res.read(int(chunk_size)) 
            i += len(chunk)
            update_pbar(pbar, i)
            data += chunk
        return data 
    else:
        return res.read()

def check_response(res):

    if type(res) == int:
        if verbose:
            print 'No usable repo found.'
        return False
    else:
        return True

def update_deb_repo(repo):

    formats = [ 'Packages.bz2', 'Packages.gz', 'Packages']
    repo_url, res = find_repo_url(repo.url, formats)
    repo.last_access_ok = check_response(res)

    if repo.last_access_ok:
        if verbose:
            print 'Found deb-style repo at: %s' % repo_url
        data = download_url(res)
        sha1 = get_sha1(data)
        if repo.file_checksum == sha1 and verbose:
            print 'Repo checksum has not changed, not updating packages'
        else:
            packages = set()
            unpack_debpackages(data, packages)
            repo.last_access_ok = True
            repo.timestamp = datetime.datetime.now()
            update_packages(repo, packages)
            repo.file_checksum = sha1
            packages.clear()
            del packages
    repo.save()

def update_rpm_repo(repo):

    formats = [ 'repomd.xml.bz2', 'repomd.xml.gz', 'repomd.xml' ]
    repo_url, res = find_repo_url(repo.url, formats)
    repo.last_access_ok = check_response(res)

    if repo.last_access_ok:
        if verbose:
            print 'Found rpm-style repo at: %s' % repo_url
        data = download_url(res)
        primary, checksum = get_primary_url(res.geturl(), data)
        res = get_repo_url(primary)
        repo.last_access_ok = check_response(res)
        if repo.last_access_ok:
            data = download_url(res)
            sha1 = get_sha1(data)
            if sha1 != checksum:
                print 'SHA1 checksum failed for repo %s, not updating packages' % repo.id
                repo.last_access_ok = False
            elif repo.file_checksum == sha1 and verbose:
                print 'Repo checksum has not changed, not updating packages'
            else:
                packages = set()
                unpack_rpmpackages(data, packages)
                repo.timestamp = datetime.datetime.now()
                repo.last_access_ok = True
                update_packages(repo, packages)
                repo.file_checksum = sha1
                packages.clear()
                del packages
    repo.save()

def update_repo(repo):
    if repo.repotype == Repository.DEB:
        update_deb_repo(repo)
    elif repo.repotype == Repository.RPM:
        update_rpm_repo(repo)
    else:
        print 'Error: unknown repo type for repo %s: %s' % (repo.id, repo.repotype)

def update_repos(single_repo):

    update_repos = []

    if single_repo:
        try:
            update_repos.append(Repository.objects.get(id=single_repo))
            message = 'Updating repository with id: %s' % single_repo
        except:
            message = 'Repo with id %s does not exist' % single_repo
    else:
        message = 'Updating all repositories'
        update_repos = Repository.objects.all()

    if verbose:
        print message

    for repo in update_repos:
        if force:
            repo.file_checksum = ''
            repo.save()
        if verbose:    
            print '\n%s' % repo
        update_repo(repo)
    del message, update_repos

def list_repos():

    print 'Defined repositories:'
    print 'id : name\n'
    for repo in Repository.objects.all():
        print '%s : %s' % (repo.id, repo)
        if verbose:
            print '%s' % repo.url
            print 'arch: %s    checksum: %s' % (repo.arch, repo.file_checksum)
            print 'security: %s  last_updated: %s\n' % (repo.security, repo.timestamp)

def remove_orphaned_packages():

    allpackages = Package.objects.all()
    allhosts = Host.objects.all()
    allrepos = Repository.objects.all()

    packagelen = len(allpackages)
    repolen = len(allrepos)
    hostlen = len(allhosts)

    allpackagestrings = set()
    allusedpackages = set()
    orphanedpackages = set()

    if verbose:
        print '\nPruning orphaned packages'

    pbar = create_pbar('Obtaining all stored packages:', packagelen)
    for i, p in enumerate(allpackages):
        allpackagestrings.add(PackageString(name=p.name, epoch=p.epoch, version=p.version, release=p.release, arch=p.arch, packagetype=p.packagetype))
        update_pbar(pbar, i+1)

    pbar = create_pbar('Obtaining repo packages:', repolen)
    for i, r in enumerate(allrepos):
        for p in r.packages.all():
            allusedpackages.add(PackageString(name=p.name, epoch=p.epoch, version=p.version, release=p.release, arch=p.arch, packagetype=p.packagetype))
        update_pbar(pbar, i+1)

    pbar = create_pbar('Obtaining host packages:', hostlen)
    for i, h in enumerate(allhosts):
        for p in h.packages.all():
            allusedpackages.add(PackageString(name=p.name, epoch=p.epoch, version=p.version, release=p.release, arch=p.arch, packagetype=p.packagetype))
        update_pbar(pbar, i+1)

    orphanedpackages = allpackagestrings.difference(allusedpackages)
    orphanedlen = len(orphanedpackages)

    pbar = create_pbar('Removing %s orphaned packages:' % orphanedlen, orphanedlen)
    for i, o in enumerate(orphanedpackages):
        try:
            p = Package.objects.get(name=o.name, epoch=o.epoch, version=o.version, release=o.release, arch=o.arch, packagetype=o.packagetype)
        except MultipleObjectsReturned:
            print "You have duplicate packages in your database for %s." % o
            print "Please re-run this tool with the duplicate-removing option first."
            exit
        p.delete()
        update_pbar(pbar, i+1)

    if orphanedlen == 0 and verbose:
        print 'No orphaned packages found.'

def remove_unused_package_names():

    allnames = PackageName.objects.all()
    allpackages = Package.objects.all()
    allnameslen = len(allnames)
    p = 0

    pbar = create_pbar('Removing unused package names:', allnameslen)
    for i, packagename in enumerate(allnames):
        update_pbar(pbar, i+1)
        if allpackages.filter(name=packagename):
            pass
        else:
            packagename.delete()
            p += 1
            
    if verbose:
        print 'Removed %s unused package names.' % p


def remove_duplicates(keep, remove):

    allhosts = Host.objects.all()
    removed = 0

    for host in allhosts:
        if remove in host.packages.all():
            host.packages.remove(remove)
            host.packages.add(keep)
            removed += 1
            host.save()
    return removed


def remove_duplicate_packages():

    allpackages = Package.objects.all()
    allpackageslen = len(allpackages)
    total_removed = 0

    pbar = create_pbar('Finding duplicate packages:', allpackageslen)

    for i, package in enumerate(allpackages):
        update_pbar(pbar, i+1)
        query = allpackages.filter(Q(name=package.name)&Q(epoch=package.epoch)&Q(version=package.version)&Q(release=package.release)&Q(arch=package.arch)&Q(packagetype=package.packagetype))
        if query.count() > 1:
            for p in query.all():
                if package.id != p.id:
                    total_removed += remove_duplicates(package, p)
                    p.delete()

    if verbose and total_removed != 0:
        print "Removed %s duplicate packages." % total_removed

def process_packages(report, host):

    packages = []

    if report.packages:
        for i, p in enumerate(report.packages.splitlines()):
            packages.append(p.replace('\'','').split(' '))
        pbar = create_pbar('%s packages' % host.__unicode__()[0:25], i) 
        for i, pkg in enumerate(packages):
            if report.protocol == '1':
                if pkg[4] != '':
                    p_arch, c = PackageArchitecture.objects.get_or_create(name=pkg[4])
                else:
                    p_arch, c = PackageArchitecture.objects.get_or_create(name='unknown')
                p_name, c = PackageName.objects.get_or_create(name=pkg[0].lower())
                if pkg[1]:
                    p_epoch = pkg[1]
                    if p_epoch == '0':
                        p_epoch = ''
                else:
                    p_epoch = ''
                p_version = pkg[2]
                if pkg[3]:
                    p_release = pkg[3]
                else:
                    p_release = ''
                p_type = Package.UNKNOWN
                if pkg[5] == 'deb':
                    p_type = Package.DEB
                if pkg[5] == 'rpm':
                    p_type = Package.RPM
                package, c = Package.objects.get_or_create(name=p_name, arch=p_arch, epoch=p_epoch, version=p_version, release=p_release, packagetype=p_type)
                host.packages.add(package)
                del package
            update_pbar(pbar, i)
        host.save()

def create_or_update_host(report):

    if report.host and report.os and report.kernel and report.domain and report.arch:
        os, c = OS.objects.get_or_create(name=report.os)
        domain, c = Domain.objects.get_or_create(name=report.domain)
        arch, c = MachineArchitecture.objects.get_or_create(name=report.arch)
        host, c = Host.objects.get_or_create(
            hostname=report.host,
            defaults={
                'ipaddress':report.report_ip,
                'arch':arch,
                'os':os,
                'domain':domain,
                'lastreport':report.time
            }
        )
        if verbose and c:
            print 'Created host %s' % host
        host.ipaddress=report.report_ip
        host.kernel=report.kernel
        host.arch=arch
        host.os=os
        host.domain=domain
        host.lastreport=report.time
        host.tags=report.tags
# TODO: fix this to use stringpackage sets to remove/add
# or queryset sets
        host.packages.clear()
        host.save()
        process_packages(report, host)
        host.save()
        return True
    else:
        return False

def process_reports(report_host):

    if report_host:
        try:
            report_hosts = Report.objects.filter(processed=force, host=report_host).order_by('time')
            message = 'Processing reports for host %s' % report_host
        except:
            message = 'No reports exist for host %s' % report_host
    else:
        message = 'Processing reports for all hosts'
        report_hosts = Report.objects.filter(processed=force).order_by('time')

    if verbose:
        print message

    for report in report_hosts:
        if create_or_update_host(report):
            report.processed = True
            report.save()


def remove_reports(host):

    reports = Report.objects.filter(host=host).order_by('time')[:3]
    report_ids = []

    for report in reports:
        report_ids.append(report.id)

    del_reports = Report.objects.filter(host=host).exclude(id__in = report_ids)

    reportlen = len(del_reports)
    if verbose:
        print host
    pbar = create_pbar('Cleaning %s old reports' % reportlen, reportlen)
    for i, report in enumerate(del_reports):
        report.delete()
        update_pbar(pbar, i+1)

def clean_reports(report_host):

    if report_host:
        remove_reports(report_host)
    else:
        for host in Host.objects.all():
            remove_reports(host)

def print_update(update, signal, *args, **kwargs):
    if verbose:
        print update

def find_host_updates(update_host):

    update_hosts = []

    if update_host:
        try:
            update_hosts.append(Host.objects.get(hostname=update_host))
            message = 'Finding updates for host %s' % update_host
        except:
            message = 'Host %s does not exist' % update_host
    else:
        message = 'Finding updates for all hosts'
        update_hosts = Host.objects.all()

    if verbose:
        print message

    for host in update_hosts:
        if verbose:
            print '\n%s' % host
        host_update_found.connect(print_update)
        host.find_updates()

def clean_updates():

    for update in PackageUpdate.objects.all():
        if update.host_set.all().count() == 0:
            if verbose:
                print 'Removing %s' % update
            update.delete()
        
if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Patchman repository tool')
    parser.add_argument('-u', '--update', action='store_true',
                help='update repositories')
    parser.add_argument('-f', '--force', action='store_true',
                help='disregard stored checksums and force-update all repositories')
    parser.add_argument('-R','--repo',
                help='only update the repository with this ID')
    parser.add_argument('-l', '--list', action='store_true',
                help='list all available repositories')
    parser.add_argument('-v', '--verbose', action='store_true',
                help='verbose output (default is to be silent, for cronjobs)')
    parser.add_argument('-o', '--orphans', action='store_true',
                help='find and remove all orphaned packages')
    parser.add_argument('-d', '--dups', action='store_true',
                help='find and remove all duplicates packages')
    parser.add_argument('-p', '--process-reports', action='store_true',
                help='process all pending reports')
    parser.add_argument('-r', '--clean-reports', action='store_true',
                help='remove all but the last three reports')
    parser.add_argument('-U', '--find-updates', action='store_true',
                help='find updates for all hosts')
    parser.add_argument('-H','--host',
                help='only find updates for this host (fqdn)')
    parser.add_argument('-c', '--clean-updates', action='store_true',
                help='find and remove updates that are no longer required') 

    args = parser.parse_args()

    force = args.force
    verbose = args.verbose

    if args.list:
        list_repos()
        exit
    if args.update:
        update_repos(args.repo)
        exit 
    if args.orphans:
        remove_orphaned_packages()
        remove_unused_package_names()
    if args.dups:
        remove_duplicate_packages()
    if args.clean_reports:
        clean_reports(args.host)
    if args.process_reports:
        process_reports(args.host)
    if args.find_updates:
        find_host_updates(args.host)
    if args.clean_updates:
        clean_updates()
