#!/usr/bin/env python

from django.core.management import setup_environ

import os
import sys
import string
import bz2
import gzip
import re
import datetime
import argparse
import math
from hashlib import sha1, sha256
from StringIO import StringIO
from lxml import etree
from debian.debian_support import Version
from debian.deb822 import Sources
from urllib2 import Request, urlopen
from urlparse import urlparse

from patchman import settings
setup_environ(settings)

from django.db.utils import IntegrityError
from django.db import connection
from django.db.models import Q, Count
from django.dispatch import Signal

from patchman.hosts.models import Host
from patchman.operatingsystems.models import OS, OSGroup
from patchman.domains.models import Domain
from patchman.packages.models import Package, PackageName, PackageString, PackageUpdate
from patchman.repos.models import Repository, MirrorPackage
from patchman.arch.models import PackageArchitecture, MachineArchitecture
from patchman.reports.models import Report

from patchman.hosts.signals import host_update_found
from patchman.signals import progress_info, progress_update, cli_message

from progressbar import Bar, ETA, Percentage, ProgressBar

force = False
verbose = False
pbar = None

def cli_message(text, **kwargs):
    print text,
    sys.stdout.softspace = False

def create_pbar(ptext, plength, **kwargs):

    global pbar
    if verbose and plength > 0:
        jtext = string.ljust(ptext, 35)
        pbar = ProgressBar(widgets=[jtext, Percentage(), Bar(), ETA()], maxval=plength).start()
        return pbar

def update_pbar(index, **kwargs):

    global pbar
    if verbose and pbar:
        pbar.update(index)
        if index == pbar.maxval:
            pbar.finish()
            pbar = None

def gunzip(contents):

    try:
        gzipdata = gzip.GzipFile(fileobj=contents)
        gzipdata = gzipdata.read()
        contents = StringIO(gzipdata)
    except IOError, e:
        import warnings
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        if e.message == 'Not a gzipped file':
            pass

    return contents.getvalue()

def bunzip2(contents):

    try:
        bzip2data = bz2.decompress(contents)
        return bzip2data
    except IOError, e:
        if e == 'invalid data stream':
            pass

def extract(data):

    extracted = bunzip2(data)
    if not extracted:
        extracted = gunzip(StringIO(data))
    return extracted

def unpack_debpackages(data, packages):

    extracted = extract(data)
    package_re = re.compile('^Package: ', re.M)
    numpackages = len(package_re.findall(extracted))

    if numpackages > 0:
        create_pbar('Extracting packages: ', numpackages)
        for i, stanza in enumerate(Sources.iter_paragraphs(StringIO(extracted))):
            fullversion = Version(stanza['version'])
            arch = stanza['architecture']
            name = stanza['package']
            epoch = fullversion._BaseVersion__epoch
            if epoch == None:
                epoch = ''
            version = fullversion._BaseVersion__upstream_version
            release = fullversion._BaseVersion__debian_revision
            if release == None:
                release = ''
            update_pbar(i+1)
            package = PackageString(name=name, epoch=epoch, version=version, release=release, arch=arch, packagetype='D')
            packages.add(package)
    else:
        if verbose:
            print 'No packages in found in repo.'

def get_primary_url(repo_url, data):

    ns = 'http://linux.duke.edu/metadata/repo'
    context = etree.parse(StringIO(data), etree.XMLParser())
    location = context.xpath("//ns:data[@type='primary']/ns:location/@href",  namespaces = {'ns': ns})[0]
    checksum = context.xpath("//ns:data[@type='primary']/ns:checksum",  namespaces = {'ns': ns})[0].text
    checksum_type = context.xpath("//ns:data[@type='primary']/ns:checksum/@type",  namespaces = {'ns': ns})[0]
    primary_url = str(repo_url.rsplit('/', 2)[0]) + '/' + location
    return primary_url, checksum, checksum_type

def get_sha1(data):

    return sha1(data).hexdigest()

def get_sha256(data):

    return sha256(data).hexdigest()

def unpack_yumpackages(data, packages):

    extracted = extract(data)
    
    ns = 'http://linux.duke.edu/metadata/common'
    context = etree.iterparse(StringIO(extracted), tag = '{%s}metadata' % ns)
    numpackages = int(context.next()[1].get('packages'))
    context = etree.iterparse(StringIO(extracted), tag = '{%s}package' % ns)

    if numpackages > 0:

        create_pbar('Extracting packages: ', numpackages)

        for i, data in enumerate(context):
            elem = data[1]
            update_pbar(i+1)
            name = elem.xpath('//ns:name', namespaces = {'ns': ns})[0].text.lower()
            arch = elem.xpath('//ns:arch', namespaces = {'ns': ns})[0].text
            fullversion = elem.xpath('//ns:version', namespaces = {'ns': ns})[0]
            epoch = fullversion.get('epoch')
            version = fullversion.get('ver')
            release = fullversion.get('rel')
            elem.clear()
            while elem.getprevious() is not None:
                del elem.getparent()[0]
    
            if name != '' and version != '' and arch != '':
                if epoch == '0':
                    epoch = ''
                package = PackageString(name=name, epoch=epoch, version=version, release=release, arch=arch, packagetype='R')
                packages.add(package)
    else:
        if verbose:
            print 'No packages in found in repo.'

def unpack_yastpackages(data, packages):

    extracted = extract(data)
    pkgs = re.findall('=Pkg: (.*)', extracted)
    numpackages = len(pkgs)

    if numpackages > 0:

        create_pbar('Extracting packages: ', numpackages)

        for i, pkg in enumerate(pkgs):
            update_pbar(i+1)
            name, version, release, arch = pkg.split()
            package = PackageString(name=name.lower(), epoch='', version=version, release=release, arch=arch, packagetype='R')
            packages.add(package)
    else:
        if verbose:
            print 'No packages in found in repo.'

def get_repo_url(url):

    try:
        req = Request(url)
        return urlopen(req)
    except IOError, e:
        if hasattr(e, 'reason'):
            print 'Failed to reach the server: %s' % e.reason
            return -1
        elif hasattr(e, 'code'):
            if verbose:
                print '%s - %s' % (url, e)
            return e.code 
        else:
            print 'Error: %s' % e
            return -1

def update_packages(mirror, packages):

    new = set()
    old = set()
    removed = set()

    repopackages = mirror.packages.all()
    reposize = len(repopackages)

    create_pbar('Obtaining stored packages: ', reposize)
    for i, package in enumerate(repopackages):
        update_pbar(i+1)
        name = str(package.name)
        arch = str(package.arch)
        strpackage = PackageString(name=name, epoch=package.epoch, version=package.version, release=package.release, arch=arch, packagetype=package.packagetype)
        old.add(strpackage)

    new = packages.difference(old)
    removed = old.difference(packages)

    old.clear()

    newlen = len(new)
    removedlen = len(removed)

    create_pbar('Removing %s obsolete packages:' % removedlen, removedlen)
    for i, package in enumerate(removed):
        update_pbar(i+1)
        package_id = PackageName.objects.get(name=package.name)
        epoch = package.epoch
        version = package.version
        release = package.release
        arch = PackageArchitecture.objects.get(name=package.arch)
        packagetype = package.packagetype
        p = Package.objects.get(name=package_id, epoch=epoch, version=version, arch=arch, release=release, packagetype=packagetype)
        MirrorPackage.objects.get(mirror=mirror, package=p).delete()
    mirror.save()
    removed.clear()

    create_pbar('Adding %s new packages:' % newlen, newlen)
    for i, package in enumerate(new):
        update_pbar(i+1)
        package_id, c = PackageName.objects.get_or_create(name=package.name)
        epoch = package.epoch
        version = package.version
        release = package.release
        packagetype = package.packagetype
        arch, c = PackageArchitecture.objects.get_or_create(name=package.arch)
        p, c = Package.objects.get_or_create(name=package_id, epoch=epoch, version=version, arch=arch, release=release, packagetype=packagetype)
        # This fixes a subtle bug where a stored package name with uppercase letters
        # will not match until it is lowercased.
        if package_id.name != package.name:
            package_id.name = package.name
            package_id.save()
        MirrorPackage.objects.create(mirror=mirror, package=p)
    mirror.save()
    new.clear()

def find_repo_url(stored_repo_url, formats):

    yast = False
    for fmt in formats:
        repo_url = stored_repo_url
        for f in formats:
            if repo_url.endswith(f):
                repo_url = repo_url[:-len(f)]
        repo_url  = repo_url.rstrip('/') + '/' + fmt
        res = get_repo_url(repo_url)
        if type(res) != int:
            break
    if fmt == 'content':
        yast = True
    return repo_url, res, yast

def download_url(res, part=''):

    headers = dict(res.headers.items())
    if verbose and 'content-length' in headers:
        if part == None:
            ptext = 'Downloading repo info:'
        else:
            ptext = 'Downloading repo info %s:' % part
        contentlen = int(headers['content-length'])
        chunk_size = 16384.0
        create_pbar(ptext, contentlen)
        i = 0
        chunks = int(math.ceil(contentlen / chunk_size))
        data = ''
        chunk = ''
        for x in range(1, chunks + 1):
            chunk = res.read(int(chunk_size)) 
            i += len(chunk)
            update_pbar(i)
            data += chunk
        return data 
    else:
        return res.read()

def check_response(res):

    if type(res) == int:
        if verbose:
            print 'No usable repo found.'
        return False
    else:
        return True

def update_deb_repo(repo):
    
    formats = [ 'Packages.bz2', 'Packages.gz', 'Packages']

    for mirror in repo.mirror_set.all():
    
        repo_url, res, unused = find_repo_url(mirror.url, formats)
        mirror.last_access_ok = check_response(res)

        if mirror.last_access_ok:
            if verbose:
                print '%s - deb repo found.' % repo_url
            data = download_url(res)
            sha1 = get_sha1(data)
            if mirror.file_checksum == sha1 and verbose:
                print 'Repo checksum has not changed, not updating packages'
            else:
                packages = set()
                unpack_debpackages(data, packages)
                mirror.last_access_ok = True
                mirror.timestamp = datetime.datetime.now()
                update_packages(mirror, packages)
                mirror.file_checksum = sha1
                packages.clear()
        mirror.save()

def update_rpm_repo(repo):

    formats = [ 'repodata/repomd.xml.bz2', 'repodata/repomd.xml.gz', 'repodata/repomd.xml', 'suse/repodata/repomd.xml.bz2', 'suse/repodata/repomd.xml.gz', 'suse/repodata/repomd.xml', 'content' ]

    for mirror in repo.mirror_set.all():
        repo_url, res, yast = find_repo_url(mirror.url, formats)
        mirror.last_access_ok = check_response(res)

        if mirror.last_access_ok:
            packages = set()
            update = False
            if not yast:
                if verbose:
                    print '%s - yum rpm repo found.' % repo_url
                data = download_url(res, '(1/2)')
                primary, checksum, checksum_type = get_primary_url(res.geturl(), data)
                res = get_repo_url(primary)
                mirror.last_access_ok = check_response(res)
                if mirror.last_access_ok:
                    data = download_url(res, '(2/2)')
                    sha = None
                    if checksum_type == 'sha':
                        sha = get_sha1(data)
                    elif checksum_type == 'sha256':
                        sha = get_sha256(data)
                    else:
                        print 'Unknown checksum type: %s' % checksum_type
                    if sha != checksum:
                        print '%s checksum failed for mirror %s, not updating packages' % (checksum_type, mirror.id)
                        mirror.last_access_ok = False
                    elif mirror.file_checksum == sha and verbose:
                        print 'Repo checksum has not changed, not updating packages'
                    else:
                        unpack_yumpackages(data, packages)
                        mirror.file_checksum = sha
                        update = True
            elif yast:
                if verbose:
                    print '%s - yast rpm repo found' % repo_url
                data = download_url(res, '(1/2)')
                rpm_dir = re.findall('DATADIR.*', data)[0].split()[1]
                rpm_url = '%s/%s' % (mirror.url, rpm_dir)
                package_dir = re.findall('DESCRDIR *(.*)', data)[0]
                package_url = '%s/%s/packages.gz' % (mirror.url, package_dir)
                res = get_repo_url(package_url)
                mirror.last_access_ok = check_response(res)
                if mirror.last_access_ok:
                    data = download_url(res, '(2/2)')
                    unpack_yastpackages(data, packages)
                    mirror.file_checksum = 'yast'
                    update = True
            mirror.timestamp = datetime.datetime.now()
            mirror.last_access_ok = True
            if update:
                update_packages(mirror, packages)
            packages.clear()

        mirror.save()

def update_repo(repo):
    if repo.repotype == Repository.DEB:
        update_deb_repo(repo)
    elif repo.repotype == Repository.RPM:
        update_rpm_repo(repo)
    else:
        print 'Error: unknown repo type for repo %s: %s' % (repo.id, repo.repotype)

def update_repos(single_repo):

    repos = []

    if single_repo:
        try:
            repos.append(Repository.objects.get(id=single_repo))
            message = 'Updating repository with id: %s' % single_repo
        except:
            message = 'Repo with id %s does not exist' % single_repo
    else:
        message = 'Updating all repositories'
        repos = Repository.objects.all()

    if verbose:
        print message

    for repo in repos:
        if force:
            repo.file_checksum = ''
            repo.save()
        if verbose:    
            print '\n%s' % repo
        update_repo(repo)

def list_repos(single_repo):

    repos = []

    if single_repo:
        try:
            repos.append(Repository.objects.get(id=single_repo))
            message = 'Repository %s info:' % single_repo
        except:
            message = 'Repo with id %s does not exist' % single_repo
    else:
        message = 'Repository information:'
        print 'id : name\n'
        repos = Repository.objects.all()

    if verbose:
        print message

    for repo in repos:
        print_repo_info(repo)

def print_repo_info(repo):

    print '%s : %s' % (repo.id, repo)
    if verbose:
        print 'security: %s  arch: %s' % (repo.security, repo.arch)
        print "Mirrors:"
        for mirror in repo.mirror_set.all():
            print mirror.url
            print 'last updated: %s    checksum: %s\n' % (mirror.timestamp, mirror.file_checksum)

def remove_orphaned_packages():

    orphanedpackages = Package.objects.filter(repository__isnull = True, host__isnull = True)
    orphanedlen = len(orphanedpackages)

    create_pbar('Removing %s orphaned packages:' % orphanedlen, orphanedlen)
    for i, o in enumerate(orphanedpackages):
        try:
            p = Package.objects.get(name=o.name, epoch=o.epoch, version=o.version, release=o.release, arch=o.arch, packagetype=o.packagetype)
        except MultipleObjectsReturned:
            print "You have duplicate packages in your database for %s." % o
            print "Please re-run this tool with the duplicate-removing option first."
            exit
        p.delete()
        update_pbar(i+1)

    if orphanedlen == 0 and verbose:
        print 'No orphaned packages found.'

def remove_unused_package_names():

    allnames = PackageName.objects.all()
    allpackages = Package.objects.all()
    allnameslen = len(allnames)
    p = 0

    create_pbar('Removing unused package names:', allnameslen)
    for i, packagename in enumerate(allnames):
        update_pbar(i+1)
        if allpackages.filter(name=packagename):
            pass
        else:
            packagename.delete()
            p += 1
            
    if verbose:
        print 'Removed %s unused package names.' % p

def remove_duplicates(keep, remove):

    allhosts = Host.objects.all()
    removed = 0

    for host in allhosts:
        if remove in host.packages.all():
            host.packages.remove(remove)
            host.packages.add(keep)
            removed += 1
            host.save()
    return removed

def remove_duplicate_packages():

    allpackages = Package.objects.all()
    allpackageslen = len(allpackages)
    total_removed = 0

    create_pbar('Finding duplicate packages:', allpackageslen)

    for i, package in enumerate(allpackages):
        update_pbar(i+1)
        query = allpackages.filter(Q(name=package.name)&Q(epoch=package.epoch)&Q(version=package.version)&Q(release=package.release)&Q(arch=package.arch)&Q(packagetype=package.packagetype))
        if query.count() > 1:
            for p in query.all():
                if package.id != p.id:
                    total_removed += remove_duplicates(package, p)
                    p.delete()

    if verbose and total_removed != 0:
        print "Removed %s duplicate packages." % total_removed

def remove_reports(host):

    reports = Report.objects.filter(host=host).order_by('time')[:3]
    report_ids = []

    for report in reports:
        report_ids.append(report.id)

    del_reports = Report.objects.filter(host=host).exclude(id__in = report_ids)

    reportlen = len(del_reports)
    if verbose:
        print host
    create_pbar('Cleaning %s old reports' % reportlen, reportlen)
    for i, report in enumerate(del_reports):
        report.delete()
        update_pbar(i+1)

def clean_reports(report_host):

    if report_host:
        remove_reports(report_host)
    else:
        for host in Host.objects.all():
            remove_reports(host)
            
def print_update(update, **kwargs):
    if verbose:
        print update

# Not needed anymore as updates are processed after each report
def find_host_updates(host=None):

    update_hosts = []

    if host:
        try:
            update_hosts.append(Host.objects.get(hostname=host))
            message = 'Finding updates for host %s' % host
        except:
            message = 'Host %s does not exist' % host
    else:
        message = 'Finding updates for all hosts'
        update_hosts = Host.objects.all()

    if verbose:
        print message

    for host in update_hosts:
        if verbose:
            print '\n%s' % host
        host_update_found.connect(print_update)
        host.find_updates()
        host_update_found.disconnect()

def process_reports(host=None):
    """ Process all pending reports, Specify host to process only a single host
    """

    report_hosts = []
    if host:
        try:
            report_hosts = Report.objects.filter(processed=force, host=host).order_by('time')
            message = 'Processing reports for host %s' % host
        except:
            message = 'No reports exist for host %s' % host
    else:
        message = 'Processing reports for all hosts'
        report_hosts = Report.objects.filter(processed=force).order_by('time')

    if verbose:
        print message

    for report in report_hosts:
        if verbose:
            progress_info.connect(create_pbar)
            progress_update.connect(update_pbar)
        report.process()
        progress_info.disconnect()
        progress_update.disconnect()
            
def clean_updates():

    for update in PackageUpdate.objects.all():
        if update.host_set.count() == 0:
            if verbose:
                print 'Removing %s' % update
            update.delete()
        
if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Patchman repository tool')
    parser.add_argument('-u', '--update', action='store_true',
                help='update repositories')
    parser.add_argument('-f', '--force', action='store_true',
                help='disregard stored checksums and force-update all repositories')
    parser.add_argument('-R', '--repo',
                help='only update the repository with this ID')
    parser.add_argument('-l', '--list', action='store_true',
                help='list all available repositories')
    parser.add_argument('-v', '--verbose', action='store_true',
                help='verbose output (default is to be silent, for cronjobs)')
    parser.add_argument('-o', '--orphans', action='store_true',
                help='find and remove all orphaned packages')
    parser.add_argument('-d', '--dups', action='store_true',
                help='find and remove all duplicates packages')
    parser.add_argument('-p', '--process-reports', action='store_true',
                help='process all pending reports')
    parser.add_argument('-r', '--clean-reports', action='store_true',
                help='remove all but the last three reports')
    parser.add_argument('-U', '--find-updates', action='store_true',
                help='find updates for all hosts')
    parser.add_argument('-H', '--host',
                help='only find updates for this host (fqdn)')
    parser.add_argument('-c', '--clean-updates', action='store_true',
                help='find and remove updates that are no longer required') 

    args = parser.parse_args()

    force = args.force
    verbose = args.verbose

    if args.list:
        list_repos(args.repo)
        exit
    if args.update:
        update_repos(args.repo)
        exit 
    if args.orphans:
        remove_orphaned_packages()
        remove_unused_package_names()
    if args.dups:
        remove_duplicate_packages()
    if args.clean_reports:
        clean_reports(args.host)
    if args.process_reports:
        process_reports(args.host)
    if args.find_updates:
        find_host_updates(args.host)
    if args.clean_updates:
        clean_updates()

